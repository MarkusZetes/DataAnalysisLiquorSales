{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c8e060",
   "metadata": {},
   "source": [
    "# Natural language processing of Shakespearean work\n",
    "\n",
    "## In this shorter project I am going to use a recurrent neural network that will generate new text based on a corpus of text data of a Shakespearen work (It has a very distinctive style. Since the text data uses old style english and is formatted in the style of a stage play, it will be very obvious to us if the model is able to reproduce similar results)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9da350",
   "metadata": {},
   "source": [
    "**The model will be built in a sequential way using a technique which I found in literature basically using an embedding layer (to turn positive integers which will be assigned to each character to vectors of probabilities of fixed sizes), a GRU unit (in this case the GRU cell tends to work better then the LSTM cell because of it's less complex build and fewer gates, thus it lacks the output gate of a classic LSTM cell) and finally a Dense layer with number of neuron = number of character classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea9114d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8eaa830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0222fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'shakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8956d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to open the file in read mode:\n",
    "text = open(path_to_file, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1c86e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d0e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understand the unique characters from the text\n",
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffe3e7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '>',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '|',\n",
       " '}']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1466cc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdbfcb9",
   "metadata": {},
   "source": [
    "- So we understand that we have 84 unique characters which can be then predicted by our model\n",
    "\n",
    "- The model can not accept string characters to then make predictions on, so I have to assign each one a specific number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e10d06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '\\n')\n",
      "(1, ' ')\n",
      "(2, '!')\n",
      "(3, '\"')\n",
      "(4, '&')\n",
      "(5, \"'\")\n",
      "(6, '(')\n",
      "(7, ')')\n",
      "(8, ',')\n",
      "(9, '-')\n",
      "(10, '.')\n",
      "(11, '0')\n",
      "(12, '1')\n",
      "(13, '2')\n",
      "(14, '3')\n",
      "(15, '4')\n",
      "(16, '5')\n",
      "(17, '6')\n",
      "(18, '7')\n",
      "(19, '8')\n",
      "(20, '9')\n",
      "(21, ':')\n",
      "(22, ';')\n",
      "(23, '<')\n",
      "(24, '>')\n",
      "(25, '?')\n",
      "(26, 'A')\n",
      "(27, 'B')\n",
      "(28, 'C')\n",
      "(29, 'D')\n",
      "(30, 'E')\n",
      "(31, 'F')\n",
      "(32, 'G')\n",
      "(33, 'H')\n",
      "(34, 'I')\n",
      "(35, 'J')\n",
      "(36, 'K')\n",
      "(37, 'L')\n",
      "(38, 'M')\n",
      "(39, 'N')\n",
      "(40, 'O')\n",
      "(41, 'P')\n",
      "(42, 'Q')\n",
      "(43, 'R')\n",
      "(44, 'S')\n",
      "(45, 'T')\n",
      "(46, 'U')\n",
      "(47, 'V')\n",
      "(48, 'W')\n",
      "(49, 'X')\n",
      "(50, 'Y')\n",
      "(51, 'Z')\n",
      "(52, '[')\n",
      "(53, ']')\n",
      "(54, '_')\n",
      "(55, '`')\n",
      "(56, 'a')\n",
      "(57, 'b')\n",
      "(58, 'c')\n",
      "(59, 'd')\n",
      "(60, 'e')\n",
      "(61, 'f')\n",
      "(62, 'g')\n",
      "(63, 'h')\n",
      "(64, 'i')\n",
      "(65, 'j')\n",
      "(66, 'k')\n",
      "(67, 'l')\n",
      "(68, 'm')\n",
      "(69, 'n')\n",
      "(70, 'o')\n",
      "(71, 'p')\n",
      "(72, 'q')\n",
      "(73, 'r')\n",
      "(74, 's')\n",
      "(75, 't')\n",
      "(76, 'u')\n",
      "(77, 'v')\n",
      "(78, 'w')\n",
      "(79, 'x')\n",
      "(80, 'y')\n",
      "(81, 'z')\n",
      "(82, '|')\n",
      "(83, '}')\n"
     ]
    }
   ],
   "source": [
    "#assign a numberr for each character\n",
    "for pair in enumerate(vocab):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424b3be",
   "metadata": {},
   "source": [
    "The logic works, but I'd rather have key and value pairs to then feed to the model, so apply the above logic to create a dictionary with the index and character pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "79106e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ind = {char:ind for ind, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28f71c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " '(': 6,\n",
       " ')': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " '0': 11,\n",
       " '1': 12,\n",
       " '2': 13,\n",
       " '3': 14,\n",
       " '4': 15,\n",
       " '5': 16,\n",
       " '6': 17,\n",
       " '7': 18,\n",
       " '8': 19,\n",
       " '9': 20,\n",
       " ':': 21,\n",
       " ';': 22,\n",
       " '<': 23,\n",
       " '>': 24,\n",
       " '?': 25,\n",
       " 'A': 26,\n",
       " 'B': 27,\n",
       " 'C': 28,\n",
       " 'D': 29,\n",
       " 'E': 30,\n",
       " 'F': 31,\n",
       " 'G': 32,\n",
       " 'H': 33,\n",
       " 'I': 34,\n",
       " 'J': 35,\n",
       " 'K': 36,\n",
       " 'L': 37,\n",
       " 'M': 38,\n",
       " 'N': 39,\n",
       " 'O': 40,\n",
       " 'P': 41,\n",
       " 'Q': 42,\n",
       " 'R': 43,\n",
       " 'S': 44,\n",
       " 'T': 45,\n",
       " 'U': 46,\n",
       " 'V': 47,\n",
       " 'W': 48,\n",
       " 'X': 49,\n",
       " 'Y': 50,\n",
       " 'Z': 51,\n",
       " '[': 52,\n",
       " ']': 53,\n",
       " '_': 54,\n",
       " '`': 55,\n",
       " 'a': 56,\n",
       " 'b': 57,\n",
       " 'c': 58,\n",
       " 'd': 59,\n",
       " 'e': 60,\n",
       " 'f': 61,\n",
       " 'g': 62,\n",
       " 'h': 63,\n",
       " 'i': 64,\n",
       " 'j': 65,\n",
       " 'k': 66,\n",
       " 'l': 67,\n",
       " 'm': 68,\n",
       " 'n': 69,\n",
       " 'o': 70,\n",
       " 'p': 71,\n",
       " 'q': 72,\n",
       " 'r': 73,\n",
       " 's': 74,\n",
       " 't': 75,\n",
       " 'u': 76,\n",
       " 'v': 77,\n",
       " 'w': 78,\n",
       " 'x': 79,\n",
       " 'y': 80,\n",
       " 'z': 81,\n",
       " '|': 82,\n",
       " '}': 83}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93d2abfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind['Z']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b7635",
   "metadata": {},
   "source": [
    "Further in the model I will rest assured need to be using the index to call a character, so to do that I will simply change the 'vocab' variable to an array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2d4f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d78ede1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_char[51]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3540eee",
   "metadata": {},
   "source": [
    "- So now this indexing works both ways, next step would be encoding the text to an integer (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8adf3420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode this character and index to the whole text:\n",
    "encoded_text = np.array([char_to_ind[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb8683cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5445609,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4553ff59",
   "metadata": {},
   "source": [
    "- We can see that this whole text dataset is extremely large, having more than 5 milion entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aac41222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When fo\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f01a4946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, 12,  0,  1,  1, 31, 73, 70, 68,  1, 61, 56, 64,\n",
       "       73, 60, 74, 75,  1, 58, 73, 60, 56, 75, 76, 73, 60, 74,  1, 78, 60,\n",
       "        1, 59, 60, 74, 64, 73, 60,  1, 64, 69, 58, 73, 60, 56, 74, 60,  8,\n",
       "        0,  1,  1, 45, 63, 56, 75,  1, 75, 63, 60, 73, 60, 57, 80,  1, 57,\n",
       "       60, 56, 76, 75, 80,  5, 74,  1, 73, 70, 74, 60,  1, 68, 64, 62, 63,\n",
       "       75,  1, 69, 60, 77, 60, 73,  1, 59, 64, 60,  8,  0,  1,  1, 27, 76,\n",
       "       75,  1, 56, 74,  1, 75, 63, 60,  1, 73, 64, 71, 60, 73,  1, 74, 63,\n",
       "       70, 76, 67, 59,  1, 57, 80,  1, 75, 64, 68, 60,  1, 59, 60, 58, 60,\n",
       "       56, 74, 60,  8,  0,  1,  1, 33, 64, 74,  1, 75, 60, 69, 59, 60, 73,\n",
       "        1, 63, 60, 64, 73,  1, 68, 64, 62, 63, 75,  1, 57, 60, 56, 73,  1,\n",
       "       63, 64, 74,  1, 68, 60, 68, 70, 73, 80, 21,  0,  1,  1, 27, 76, 75,\n",
       "        1, 75, 63, 70, 76,  1, 58, 70, 69, 75, 73, 56, 58, 75, 60, 59,  1,\n",
       "       75, 70,  1, 75, 63, 64, 69, 60,  1, 70, 78, 69,  1, 57, 73, 64, 62,\n",
       "       63, 75,  1, 60, 80, 60, 74,  8,  0,  1,  1, 31, 60, 60, 59,  5, 74,\n",
       "       75,  1, 75, 63, 80,  1, 67, 64, 62, 63, 75,  5, 74,  1, 61, 67, 56,\n",
       "       68, 60,  1, 78, 64, 75, 63,  1, 74, 60, 67, 61,  9, 74, 76, 57, 74,\n",
       "       75, 56, 69, 75, 64, 56, 67,  1, 61, 76, 60, 67,  8,  0,  1,  1, 38,\n",
       "       56, 66, 64, 69, 62,  1, 56,  1, 61, 56, 68, 64, 69, 60,  1, 78, 63,\n",
       "       60, 73, 60,  1, 56, 57, 76, 69, 59, 56, 69, 58, 60,  1, 67, 64, 60,\n",
       "       74,  8,  0,  1,  1, 45, 63, 80,  1, 74, 60, 67, 61,  1, 75, 63, 80,\n",
       "        1, 61, 70, 60,  8,  1, 75, 70,  1, 75, 63, 80,  1, 74, 78, 60, 60,\n",
       "       75,  1, 74, 60, 67, 61,  1, 75, 70, 70,  1, 58, 73, 76, 60, 67, 21,\n",
       "        0,  1,  1, 45, 63, 70, 76,  1, 75, 63, 56, 75,  1, 56, 73, 75,  1,\n",
       "       69, 70, 78,  1, 75, 63, 60,  1, 78, 70, 73, 67, 59,  5, 74,  1, 61,\n",
       "       73, 60, 74, 63,  1, 70, 73, 69, 56, 68, 60, 69, 75,  8,  0,  1,  1,\n",
       "       26, 69, 59,  1, 70, 69, 67, 80,  1, 63, 60, 73, 56, 67, 59,  1, 75,\n",
       "       70,  1, 75, 63, 60,  1, 62, 56, 76, 59, 80,  1, 74, 71, 73, 64, 69,\n",
       "       62,  8,  0,  1,  1, 48, 64, 75, 63, 64, 69,  1, 75, 63, 64, 69, 60,\n",
       "        1, 70, 78, 69,  1, 57, 76, 59,  1, 57, 76, 73, 64, 60, 74, 75,  1,\n",
       "       75, 63, 80,  1, 58, 70, 69, 75, 60, 69, 75,  8,  0,  1,  1, 26, 69,\n",
       "       59,  1, 75, 60, 69, 59, 60, 73,  1, 58, 63, 76, 73, 67,  1, 68, 56,\n",
       "       66,  5, 74, 75,  1, 78, 56, 74, 75, 60,  1, 64, 69,  1, 69, 64, 62,\n",
       "       62, 56, 73, 59, 64, 69, 62, 21,  0,  1,  1,  1,  1, 41, 64, 75, 80,\n",
       "        1, 75, 63, 60,  1, 78, 70, 73, 67, 59,  8,  1, 70, 73,  1, 60, 67,\n",
       "       74, 60,  1, 75, 63, 64, 74,  1, 62, 67, 76, 75, 75, 70, 69,  1, 57,\n",
       "       60,  8,  0,  1,  1,  1,  1, 45, 70,  1, 60, 56, 75,  1, 75, 63, 60,\n",
       "        1, 78, 70, 73, 67, 59,  5, 74,  1, 59, 76, 60,  8,  1, 57, 80,  1,\n",
       "       75, 63, 60,  1, 62, 73, 56, 77, 60,  1, 56, 69, 59,  1, 75, 63, 60,\n",
       "       60, 10,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, 13,  0,  1,  1, 48, 63, 60, 69,\n",
       "        1, 61, 70])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7f03eb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      "                     2\n",
      "  When fo\n"
     ]
    }
   ],
   "source": [
    "print(text[:700])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a58f0",
   "metadata": {},
   "source": [
    "It is important in Natural Language Processing to feed the information in batches. Batches should contain all neccesary information that the model could get about this text, but they shouldn't be too long, else it will be very long and hard to train the model.\n",
    "\n",
    "This work of shakespeares has all basic information it needs in max 3 lines, because every second lines rhymes and has unique structural aspects. So 3 lines would be ok to be seen as a sequence of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b05c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = '''From fairest creatures we desire increase,\n",
    "  That thereby beauty's rose might never die,\n",
    "  But as the riper should by time decease,'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61982bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea7b111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 120 #Close to 130 but given all the white spaces it suits ok with 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4acd228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See how many sequences of 120 we have (seq_len + 1 because of 0 indexing)\n",
    "total_num_seq = len(text) // (seq_len+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed5220e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45005"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ecebf",
   "metadata": {},
   "source": [
    "Next step is creating the actual training sequences as a dataset and luckily tensorflow has a Dataset object that can be used exactly for this kind of task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "336a6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3487591c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.from_tensor_slices_op.TensorSliceDataset"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(char_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fb37a171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "12\n",
      "0\n",
      "1\n",
      "1\n",
      "31\n",
      "73\n",
      "70\n",
      "68\n",
      "1\n",
      "61\n",
      "56\n",
      "64\n",
      "73\n",
      "60\n",
      "74\n",
      "75\n",
      "1\n",
      "58\n",
      "73\n",
      "60\n",
      "56\n",
      "75\n",
      "76\n",
      "73\n",
      "60\n",
      "74\n",
      "1\n",
      "78\n",
      "60\n",
      "1\n",
      "59\n",
      "60\n",
      "74\n",
      "64\n",
      "73\n",
      "60\n",
      "1\n",
      "64\n",
      "69\n",
      "58\n",
      "73\n",
      "60\n",
      "56\n",
      "74\n",
      "60\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "45\n",
      "63\n",
      "56\n",
      "75\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "73\n",
      "60\n",
      "57\n",
      "80\n",
      "1\n",
      "57\n",
      "60\n",
      "56\n",
      "76\n",
      "75\n",
      "80\n",
      "5\n",
      "74\n",
      "1\n",
      "73\n",
      "70\n",
      "74\n",
      "60\n",
      "1\n",
      "68\n",
      "64\n",
      "62\n",
      "63\n",
      "75\n",
      "1\n",
      "69\n",
      "60\n",
      "77\n",
      "60\n",
      "73\n",
      "1\n",
      "59\n",
      "64\n",
      "60\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "27\n",
      "76\n",
      "75\n",
      "1\n",
      "56\n",
      "74\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "1\n",
      "73\n",
      "64\n",
      "71\n",
      "60\n",
      "73\n",
      "1\n",
      "74\n",
      "63\n",
      "70\n",
      "76\n",
      "67\n",
      "59\n",
      "1\n",
      "57\n",
      "80\n",
      "1\n",
      "75\n",
      "64\n",
      "68\n",
      "60\n",
      "1\n",
      "59\n",
      "60\n",
      "58\n",
      "60\n",
      "56\n",
      "74\n",
      "60\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "33\n",
      "64\n",
      "74\n",
      "1\n",
      "75\n",
      "60\n",
      "69\n",
      "59\n",
      "60\n",
      "73\n",
      "1\n",
      "63\n",
      "60\n",
      "64\n",
      "73\n",
      "1\n",
      "68\n",
      "64\n",
      "62\n",
      "63\n",
      "75\n",
      "1\n",
      "57\n",
      "60\n",
      "56\n",
      "73\n",
      "1\n",
      "63\n",
      "64\n",
      "74\n",
      "1\n",
      "68\n",
      "60\n",
      "68\n",
      "70\n",
      "73\n",
      "80\n",
      "21\n",
      "0\n",
      "1\n",
      "1\n",
      "27\n",
      "76\n",
      "75\n",
      "1\n",
      "75\n",
      "63\n",
      "70\n",
      "76\n",
      "1\n",
      "58\n",
      "70\n",
      "69\n",
      "75\n",
      "73\n",
      "56\n",
      "58\n",
      "75\n",
      "60\n",
      "59\n",
      "1\n",
      "75\n",
      "70\n",
      "1\n",
      "75\n",
      "63\n",
      "64\n",
      "69\n",
      "60\n",
      "1\n",
      "70\n",
      "78\n",
      "69\n",
      "1\n",
      "57\n",
      "73\n",
      "64\n",
      "62\n",
      "63\n",
      "75\n",
      "1\n",
      "60\n",
      "80\n",
      "60\n",
      "74\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "31\n",
      "60\n",
      "60\n",
      "59\n",
      "5\n",
      "74\n",
      "75\n",
      "1\n",
      "75\n",
      "63\n",
      "80\n",
      "1\n",
      "67\n",
      "64\n",
      "62\n",
      "63\n",
      "75\n",
      "5\n",
      "74\n",
      "1\n",
      "61\n",
      "67\n",
      "56\n",
      "68\n",
      "60\n",
      "1\n",
      "78\n",
      "64\n",
      "75\n",
      "63\n",
      "1\n",
      "74\n",
      "60\n",
      "67\n",
      "61\n",
      "9\n",
      "74\n",
      "76\n",
      "57\n",
      "74\n",
      "75\n",
      "56\n",
      "69\n",
      "75\n",
      "64\n",
      "56\n",
      "67\n",
      "1\n",
      "61\n",
      "76\n",
      "60\n",
      "67\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "38\n",
      "56\n",
      "66\n",
      "64\n",
      "69\n",
      "62\n",
      "1\n",
      "56\n",
      "1\n",
      "61\n",
      "56\n",
      "68\n",
      "64\n",
      "69\n",
      "60\n",
      "1\n",
      "78\n",
      "63\n",
      "60\n",
      "73\n",
      "60\n",
      "1\n",
      "56\n",
      "57\n",
      "76\n",
      "69\n",
      "59\n",
      "56\n",
      "69\n",
      "58\n",
      "60\n",
      "1\n",
      "67\n",
      "64\n",
      "60\n",
      "74\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "45\n",
      "63\n",
      "80\n",
      "1\n",
      "74\n",
      "60\n",
      "67\n",
      "61\n",
      "1\n",
      "75\n",
      "63\n",
      "80\n",
      "1\n",
      "61\n",
      "70\n",
      "60\n",
      "8\n",
      "1\n",
      "75\n",
      "70\n",
      "1\n",
      "75\n",
      "63\n",
      "80\n",
      "1\n",
      "74\n",
      "78\n",
      "60\n",
      "60\n",
      "75\n",
      "1\n",
      "74\n",
      "60\n",
      "67\n",
      "61\n",
      "1\n",
      "75\n",
      "70\n",
      "70\n",
      "1\n",
      "58\n",
      "73\n",
      "76\n",
      "60\n",
      "67\n",
      "21\n",
      "0\n",
      "1\n",
      "1\n",
      "45\n",
      "63\n",
      "70\n",
      "76\n",
      "1\n",
      "75\n",
      "63\n",
      "56\n",
      "75\n",
      "1\n",
      "56\n",
      "73\n",
      "75\n",
      "1\n",
      "69\n",
      "70\n",
      "78\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "1\n",
      "78\n",
      "70\n",
      "73\n",
      "67\n",
      "59\n",
      "5\n",
      "74\n",
      "1\n",
      "61\n",
      "73\n",
      "60\n",
      "74\n",
      "63\n",
      "1\n",
      "70\n",
      "73\n",
      "69\n",
      "56\n",
      "68\n",
      "60\n",
      "69\n",
      "75\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "26\n",
      "69\n",
      "59\n",
      "1\n",
      "70\n",
      "69\n",
      "67\n",
      "80\n",
      "1\n",
      "63\n",
      "60\n",
      "73\n",
      "56\n",
      "67\n",
      "59\n",
      "1\n",
      "75\n",
      "70\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "1\n",
      "62\n",
      "56\n",
      "76\n",
      "59\n",
      "80\n",
      "1\n",
      "74\n",
      "71\n",
      "73\n",
      "64\n",
      "69\n",
      "62\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "48\n",
      "64\n",
      "75\n",
      "63\n",
      "64\n",
      "69\n",
      "1\n",
      "75\n",
      "63\n",
      "64\n",
      "69\n",
      "60\n",
      "1\n",
      "70\n",
      "78\n",
      "69\n",
      "1\n",
      "57\n",
      "76\n",
      "59\n",
      "1\n",
      "57\n",
      "76\n",
      "73\n",
      "64\n",
      "60\n",
      "74\n",
      "75\n",
      "1\n",
      "75\n",
      "63\n",
      "80\n",
      "1\n",
      "58\n",
      "70\n",
      "69\n",
      "75\n",
      "60\n",
      "69\n",
      "75\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "26\n",
      "69\n",
      "59\n",
      "1\n",
      "75\n",
      "60\n",
      "69\n",
      "59\n",
      "60\n",
      "73\n",
      "1\n",
      "58\n",
      "63\n",
      "76\n",
      "73\n",
      "67\n",
      "1\n",
      "68\n",
      "56\n",
      "66\n",
      "5\n",
      "74\n",
      "75\n",
      "1\n",
      "78\n",
      "56\n",
      "74\n",
      "75\n",
      "60\n",
      "1\n",
      "64\n",
      "69\n",
      "1\n",
      "69\n",
      "64\n",
      "62\n",
      "62\n",
      "56\n",
      "73\n",
      "59\n",
      "64\n",
      "69\n",
      "62\n",
      "21\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "41\n",
      "64\n",
      "75\n",
      "80\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "1\n",
      "78\n",
      "70\n",
      "73\n",
      "67\n",
      "59\n",
      "8\n",
      "1\n",
      "70\n",
      "73\n",
      "1\n",
      "60\n",
      "67\n",
      "74\n",
      "60\n",
      "1\n",
      "75\n",
      "63\n",
      "64\n",
      "74\n",
      "1\n",
      "62\n",
      "67\n",
      "76\n",
      "75\n",
      "75\n",
      "70\n",
      "69\n",
      "1\n",
      "57\n",
      "60\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "45\n",
      "70\n",
      "1\n",
      "60\n",
      "56\n",
      "75\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "1\n",
      "78\n",
      "70\n",
      "73\n",
      "67\n",
      "59\n",
      "5\n",
      "74\n",
      "1\n",
      "59\n",
      "76\n",
      "60\n",
      "8\n",
      "1\n",
      "57\n",
      "80\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "1\n",
      "62\n",
      "73\n",
      "56\n",
      "77\n",
      "60\n",
      "1\n",
      "56\n",
      "69\n",
      "59\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "60\n",
      "10\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "13\n",
      "0\n",
      "1\n",
      "1\n",
      "48\n",
      "63\n",
      "60\n",
      "69\n",
      "1\n",
      "61\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "#Example of what this future call will do:\n",
    "for item in char_dataset.take(700):\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a4dd6769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "1\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "F\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "f\n",
      "a\n",
      "i\n",
      "r\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "c\n",
      "r\n",
      "e\n",
      "a\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      "s\n",
      " \n",
      "w\n",
      "e\n",
      " \n",
      "d\n",
      "e\n",
      "s\n",
      "i\n",
      "r\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "r\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "T\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      "b\n",
      "y\n",
      " \n",
      "b\n",
      "e\n",
      "a\n",
      "u\n",
      "t\n",
      "y\n",
      "'\n",
      "s\n",
      " \n",
      "r\n",
      "o\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "n\n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      " \n",
      "d\n",
      "i\n",
      "e\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "B\n",
      "u\n",
      "t\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "r\n",
      "i\n",
      "p\n",
      "e\n",
      "r\n",
      " \n",
      "s\n",
      "h\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      " \n",
      "d\n",
      "e\n",
      "c\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "H\n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "e\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "m\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "b\n",
      "e\n",
      "a\n",
      "r\n",
      " \n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "m\n",
      "e\n",
      "m\n",
      "o\n",
      "r\n",
      "y\n",
      ":\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "B\n",
      "u\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "o\n",
      "u\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "t\n",
      "r\n",
      "a\n",
      "c\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "e\n",
      " \n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "b\n",
      "r\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "e\n",
      "y\n",
      "e\n",
      "s\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "F\n",
      "e\n",
      "e\n",
      "d\n",
      "'\n",
      "s\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "y\n",
      " \n",
      "l\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      "'\n",
      "s\n",
      " \n",
      "f\n",
      "l\n",
      "a\n",
      "m\n",
      "e\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      "-\n",
      "s\n",
      "u\n",
      "b\n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      "t\n",
      "i\n",
      "a\n",
      "l\n",
      " \n",
      "f\n",
      "u\n",
      "e\n",
      "l\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "M\n",
      "a\n",
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      " \n",
      "f\n",
      "a\n",
      "m\n",
      "i\n",
      "n\n",
      "e\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "b\n",
      "u\n",
      "n\n",
      "d\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "l\n",
      "i\n",
      "e\n",
      "s\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "T\n",
      "h\n",
      "y\n",
      " \n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "y\n",
      " \n",
      "f\n",
      "o\n",
      "e\n",
      ",\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "y\n",
      " \n",
      "s\n",
      "w\n",
      "e\n",
      "e\n",
      "t\n",
      " \n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      " \n",
      "t\n",
      "o\n",
      "o\n",
      " \n",
      "c\n",
      "r\n",
      "u\n",
      "e\n",
      "l\n",
      ":\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "T\n",
      "h\n",
      "o\n",
      "u\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "a\n",
      "r\n",
      "t\n",
      " \n",
      "n\n",
      "o\n",
      "w\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      "'\n",
      "s\n",
      " \n",
      "f\n",
      "r\n",
      "e\n",
      "s\n",
      "h\n",
      " \n",
      "o\n",
      "r\n",
      "n\n",
      "a\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "A\n",
      "n\n",
      "d\n",
      " \n",
      "o\n",
      "n\n",
      "l\n",
      "y\n",
      " \n",
      "h\n",
      "e\n",
      "r\n",
      "a\n",
      "l\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "g\n",
      "a\n",
      "u\n",
      "d\n",
      "y\n",
      " \n",
      "s\n",
      "p\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "W\n",
      "i\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "e\n",
      " \n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "b\n",
      "u\n",
      "d\n",
      " \n",
      "b\n",
      "u\n",
      "r\n",
      "i\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "y\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "t\n",
      "e\n",
      "n\n",
      "t\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "A\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "e\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "c\n",
      "h\n",
      "u\n",
      "r\n",
      "l\n",
      " \n",
      "m\n",
      "a\n",
      "k\n",
      "'\n",
      "s\n",
      "t\n",
      " \n",
      "w\n",
      "a\n",
      "s\n",
      "t\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "n\n",
      "i\n",
      "g\n",
      "g\n",
      "a\n",
      "r\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      ":\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "P\n",
      "i\n",
      "t\n",
      "y\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      ",\n",
      " \n",
      "o\n",
      "r\n",
      " \n",
      "e\n",
      "l\n",
      "s\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "g\n",
      "l\n",
      "u\n",
      "t\n",
      "t\n",
      "o\n",
      "n\n",
      " \n",
      "b\n",
      "e\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "T\n",
      "o\n",
      " \n",
      "e\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      "'\n",
      "s\n",
      " \n",
      "d\n",
      "u\n",
      "e\n",
      ",\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "g\n",
      "r\n",
      "a\n",
      "v\n",
      "e\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "e\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "2\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "W\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "f\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "# as charcters\n",
    "for item in char_dataset.take(700):\n",
    "    print(ind_to_char[item.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2269379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_len + 1, drop_remainder=True) \n",
    "\n",
    "#'drop_reminder = True' simply just drops the last sequences (45005 is not divisible with 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "726ad81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create target text sequence\n",
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1] \n",
    "    target_txt = seq[1:] \n",
    "    return input_txt, target_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6fe72",
   "metadata": {},
   "source": [
    "What this function actually does is that it shifts the sequence with 1 predicted character into the future: \n",
    "\n",
    "Example: Hello my name i -----> ello my name is\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa5cb21",
   "metadata": {},
   "source": [
    "This above function executes the command only for 1 sequence, but I need to map the function to all sequences in my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8111ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14960f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
      "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
      "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
      " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
      " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But\n",
      "\n",
      "\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
      "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
      " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
      " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
      "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But \n"
     ]
    }
   ],
   "source": [
    "#Example for how this looks like for 1 batch:\n",
    "for input_txt, target_txt in  dataset.take(1):\n",
    "    print(input_txt.numpy())\n",
    "    print(''.join(ind_to_char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    # There is an extra whitespace!\n",
    "    print(''.join(ind_to_char[target_txt.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5401c1",
   "metadata": {},
   "source": [
    "In the above lines I exemplified what this function actually does on our text. In the numeric outputs can one really see that in the target text the 0 acutally disappears and at the end a 1 is appended (meaning a whitespace) that is why in the character output it's hard to be seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dced900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the actual training batches that the model receives\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1433f389",
   "metadata": {},
   "source": [
    "- the dataset needs to be shuffled so that the model will train well, given the fact that I will not use some extremely complicated and fancy model, because my laptop can't take that honestly (given the extremely high number of entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6489ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c635902a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(128, 120), dtype=tf.int32, name=None), TensorSpec(shape=(128, 120), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a1950",
   "metadata": {},
   "source": [
    "We can now clearly see that the dataset has as input 128 sequences of 120 characters each and a target of the same size for each batch that it feeds to the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0590005b",
   "metadata": {},
   "source": [
    "Next I'm going to declare a few variables that we need for the model building, they are extremely intuitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfdda205",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abab01ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be1744a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose embedding dimensions in the range of the vocab_size, but not extremely large\n",
    "embed_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a6dd031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will have a single layer, so we will populate it with many neurons in order for the model to at least try and train well\n",
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c4a86b",
   "metadata": {},
   "source": [
    "The next step can be a bit tricky, hence I want to use the sparse_categorical_crossentropy (beacsue my labels are one hot encoded), but as default this loss function has 'from_logits' parameter set as 'false' (menaning that the values are not one hot encoded) and I want to set it to true. I can't set it later manually because I will only call this function in another function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "057a7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b11ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true, y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69e10b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78b4360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "     \n",
    "    #Adding the embedding layer with the parameters already delimited\n",
    "        \n",
    "    model.add(Embedding(vocab_size, embed_dim, batch_input_shape = [batch_size, None]))\n",
    "    \n",
    "    #Adding the GRU unit\n",
    "    \n",
    "    model.add(GRU(rnn_neurons, return_sequences= True, stateful=True,\n",
    "                 recurrent_initializer= 'glorot_uniform')) \n",
    "    \n",
    "    #the recurrent initializer has the orthogonal function as default, but I read that the glorot (or xavier) uniformization works better\n",
    "    \n",
    "    #Output layer\n",
    "    \n",
    "    model.add(Dense(vocab_size))\n",
    "    \n",
    "    # As usual, using the Adam optimizer as it is shown many many times that is has the best performance via Gradient Descent\n",
    "    \n",
    "    model.compile('adam', loss = sparse_cat_loss) #This is why we changed the function earlier, beacuse we cannot modifiy it here\n",
    "    \n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10e4afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model\n",
    "\n",
    "model = create_model(vocab_size=vocab_size, embed_dim=embed_dim,\n",
    "                    rnn_neurons=rnn_neurons,\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92314757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example for the model without any training, to showcase that it really grabs random characters\n",
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    \n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "295fc08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 120, 84])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6adccf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(120, 84), dtype=float32, numpy=\n",
       "array([[ 0.00063192, -0.00364161, -0.00174475, ..., -0.00661242,\n",
       "        -0.00419879, -0.00387674],\n",
       "       [-0.00342538, -0.00333595, -0.00148729, ..., -0.00127263,\n",
       "        -0.00338095, -0.00265343],\n",
       "       [ 0.00429707, -0.00058925, -0.00121954, ..., -0.00205248,\n",
       "        -0.00184891, -0.0065268 ],\n",
       "       ...,\n",
       "       [-0.00096255, -0.00537518, -0.00208286, ...,  0.00387456,\n",
       "         0.00804729, -0.00029055],\n",
       "       [-0.00174261, -0.00204345, -0.00303095, ...,  0.0023606 ,\n",
       "         0.00609189,  0.00099497],\n",
       "       [ 0.00366283,  0.00192956, -0.00114651, ...,  0.00050182,\n",
       "         0.00175337, -0.00514684]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions[0] #this are probabilities that it assigns for each concuret charatcer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92adab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform into the wanted integers\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d1a167e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(120, 1), dtype=int64, numpy=\n",
       "array([[58],\n",
       "       [61],\n",
       "       [38],\n",
       "       [26],\n",
       "       [35],\n",
       "       [25],\n",
       "       [ 1],\n",
       "       [20],\n",
       "       [61],\n",
       "       [58],\n",
       "       [32],\n",
       "       [39],\n",
       "       [62],\n",
       "       [36],\n",
       "       [40],\n",
       "       [ 7],\n",
       "       [72],\n",
       "       [65],\n",
       "       [21],\n",
       "       [46],\n",
       "       [78],\n",
       "       [75],\n",
       "       [64],\n",
       "       [81],\n",
       "       [74],\n",
       "       [17],\n",
       "       [52],\n",
       "       [51],\n",
       "       [70],\n",
       "       [24],\n",
       "       [77],\n",
       "       [71],\n",
       "       [70],\n",
       "       [26],\n",
       "       [56],\n",
       "       [21],\n",
       "       [20],\n",
       "       [ 1],\n",
       "       [76],\n",
       "       [74],\n",
       "       [38],\n",
       "       [10],\n",
       "       [34],\n",
       "       [19],\n",
       "       [32],\n",
       "       [56],\n",
       "       [79],\n",
       "       [36],\n",
       "       [59],\n",
       "       [ 3],\n",
       "       [39],\n",
       "       [56],\n",
       "       [68],\n",
       "       [55],\n",
       "       [70],\n",
       "       [69],\n",
       "       [49],\n",
       "       [54],\n",
       "       [21],\n",
       "       [62],\n",
       "       [82],\n",
       "       [70],\n",
       "       [65],\n",
       "       [57],\n",
       "       [56],\n",
       "       [83],\n",
       "       [67],\n",
       "       [ 8],\n",
       "       [17],\n",
       "       [ 1],\n",
       "       [21],\n",
       "       [68],\n",
       "       [19],\n",
       "       [54],\n",
       "       [60],\n",
       "       [40],\n",
       "       [11],\n",
       "       [20],\n",
       "       [79],\n",
       "       [ 4],\n",
       "       [75],\n",
       "       [63],\n",
       "       [81],\n",
       "       [67],\n",
       "       [62],\n",
       "       [68],\n",
       "       [28],\n",
       "       [37],\n",
       "       [ 7],\n",
       "       [43],\n",
       "       [47],\n",
       "       [ 4],\n",
       "       [45],\n",
       "       [35],\n",
       "       [36],\n",
       "       [44],\n",
       "       [46],\n",
       "       [53],\n",
       "       [45],\n",
       "       [ 2],\n",
       "       [16],\n",
       "       [29],\n",
       "       [80],\n",
       "       [20],\n",
       "       [51],\n",
       "       [36],\n",
       "       [74],\n",
       "       [12],\n",
       "       [42],\n",
       "       [21],\n",
       "       [60],\n",
       "       [ 5],\n",
       "       [55],\n",
       "       [58],\n",
       "       [18],\n",
       "       [14],\n",
       "       [40],\n",
       "       [ 3],\n",
       "       [42],\n",
       "       [48]], dtype=int64)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f350b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape the given indeces in a way in which later we can grab the characters from them\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis = -1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "976b3d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58, 61, 38, 26, 35, 25,  1, 20, 61, 58, 32, 39, 62, 36, 40,  7, 72,\n",
       "       65, 21, 46, 78, 75, 64, 81, 74, 17, 52, 51, 70, 24, 77, 71, 70, 26,\n",
       "       56, 21, 20,  1, 76, 74, 38, 10, 34, 19, 32, 56, 79, 36, 59,  3, 39,\n",
       "       56, 68, 55, 70, 69, 49, 54, 21, 62, 82, 70, 65, 57, 56, 83, 67,  8,\n",
       "       17,  1, 21, 68, 19, 54, 60, 40, 11, 20, 79,  4, 75, 63, 81, 67, 62,\n",
       "       68, 28, 37,  7, 43, 47,  4, 45, 35, 36, 44, 46, 53, 45,  2, 16, 29,\n",
       "       80, 20, 51, 36, 74, 12, 42, 21, 60,  5, 55, 58, 18, 14, 40,  3, 42,\n",
       "       48], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ccb5a41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['c', 'f', 'M', 'A', 'J', '?', ' ', '9', 'f', 'c', 'G', 'N', 'g',\n",
       "       'K', 'O', ')', 'q', 'j', ':', 'U', 'w', 't', 'i', 'z', 's', '6',\n",
       "       '[', 'Z', 'o', '>', 'v', 'p', 'o', 'A', 'a', ':', '9', ' ', 'u',\n",
       "       's', 'M', '.', 'I', '8', 'G', 'a', 'x', 'K', 'd', '\"', 'N', 'a',\n",
       "       'm', '`', 'o', 'n', 'X', '_', ':', 'g', '|', 'o', 'j', 'b', 'a',\n",
       "       '}', 'l', ',', '6', ' ', ':', 'm', '8', '_', 'e', 'O', '0', '9',\n",
       "       'x', '&', 't', 'h', 'z', 'l', 'g', 'm', 'C', 'L', ')', 'R', 'V',\n",
       "       '&', 'T', 'J', 'K', 'S', 'U', ']', 'T', '!', '5', 'D', 'y', '9',\n",
       "       'Z', 'K', 's', '1', 'Q', ':', 'e', \"'\", '`', 'c', '7', '3', 'O',\n",
       "       '\"', 'Q', 'W'], dtype='<U1')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_char[sampled_indices] #a bunch of random predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323805bc",
   "metadata": {},
   "source": [
    "- I just wanted to showcase in the above few lines that without training the model gives out entirily random predictions, so the shuffeling works well and also I wanted to compare the performance of the model after training with these random generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "684a2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "# epochs = 30\n",
    "\n",
    "# model.fit(dataset, epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47865df7",
   "metadata": {},
   "source": [
    "Training takes a really long time because of my GPU, so I had someone run my model on google colab on better GPU and I will evaluate the already finished model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "478b91c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0f6a7",
   "metadata": {},
   "source": [
    "- After loading the model, I wanted to change up the output shape, because I want it to output a single batch of text, not 128 as before\n",
    "\n",
    "- That is the reason that I load only the weights of the model\n",
    "\n",
    "- And I want to create a function that generates text based on the outputs of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ccbcc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size = 1)\n",
    "\n",
    "model.load_weights('shakespeare_model.h5')\n",
    "\n",
    "model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe4d830c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (1, None, 64)             5376      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (1, None, 1026)           3361176   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 84)             86268     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2aee913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the function that generates text:\n",
    "def text_generator(model, start_seed, gen_size = 700, temp = 0.9):\n",
    "    \n",
    "    #Number of characters to generate (could be really anything)\n",
    "    \n",
    "    num_generate = gen_size\n",
    "    \n",
    "    #For every character I will transform it to index and create a list of those\n",
    "    #Basically vectorizing the starting seed text\n",
    "    input_txt = [char_to_ind[i] for i in start_seed]\n",
    "    \n",
    "    #Expand to match the batch format shape\n",
    "    \n",
    "    input_txt = tf.expand_dims(input_txt, 0)\n",
    "    \n",
    "    #Empty list to hold the generated text:\n",
    "    \n",
    "    gen_text = []\n",
    "    \n",
    "    #Adding the temperature which is a parameter that effects the randomness of the resulting text\n",
    "    \n",
    "    #It effects the probability of next characters\n",
    "    \n",
    "    temperature = temp\n",
    "    \n",
    "    #Again making sure that the batch_size == 1\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    for nr in range(num_generate):\n",
    "        \n",
    "        #Generate predictions\n",
    "        \n",
    "        preds = model(input_txt)\n",
    "        \n",
    "        #I have to remove the batch shape dimension (reverse the expand command)\n",
    "        preds = tf.squeeze(preds, 0)\n",
    "        \n",
    "        #I want to use a categorical distribution to select the next character\n",
    "        predictions = preds / temperature\n",
    "        predicted_ind = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        #Passing the predicted character dor the next input\n",
    "        \n",
    "        input_txt = tf.expand_dims([predicted_ind], 0)\n",
    "        \n",
    "        #Transforming back to character (not index)\n",
    "        \n",
    "        gen_text.append(ind_to_char[predicted_ind])\n",
    "        \n",
    "    return (start_seed + ''.join(gen_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e452520",
   "metadata": {},
   "source": [
    "For the last part let's actually generate text with this model:\n",
    "\n",
    "- we can pass in any text that should appear in a Shakespeare work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4eea7619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lovery for one foot\n",
      "    faintes driven by a matter Prosecubber, he would not be.\n",
      "  POLIXENES. I shall?  \n",
      "  STEPHANO. But that the love is too young players of this form.\n",
      "  JESSICE. I would I had by it be here went Troy, sweet bind win\n",
      "    A most indulated flower mutiny.\n",
      "  LADY MACBETH. Your answer, sir.\n",
      "  AUTOLYCUS. Very true. And you will do it, and hear to the sheep -\n",
      "    I'll to the Queen, and the ships for his lord,\n",
      "    Which makes him gasp as tenderly away.\n",
      "  ROSALINE. Who were best been sickness?\n",
      "  PAULINA. Ay, and look so. His face is like that sleeve.\n",
      "  SATURNINUS. A goodly hard hour mine commotion,\n",
      "    In some e both present money. I,\n",
      "    Make this forever, which not the ship lies let h\n"
     ]
    }
   ],
   "source": [
    "print(text_generator(model, 'love', gen_size = 700))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca41d1e",
   "metadata": {},
   "source": [
    "## From my point of view, it's really cool that such an easy model is able to learn on a character by character basis and generate shakesperean text, where it can delimit character names in all caps and what they say, and the sentances really make sense. Surely there are made up words and typos, but all in all I am satisfied by the text generation of this model. \n",
    "\n",
    "# Thank you for you attention, hope you enjoyed it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc958b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
